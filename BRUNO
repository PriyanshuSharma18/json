import re
from bs4 import BeautifulSoup

# --- Canonical field names we finally want in JSON ---
CANONICAL_FIELDS = [
    "station",
    "weatherPhenomenon",
    "operationProbability",
    "advisoryTimePeriodStartUTC",
    "advisoryTimePeriodEndUTC",
]


def normalize_text(s: str) -> str:
    """
    Heavy normalization for matching:
    - lowercase
    - remove all non-alphanumeric chars
    """
    if s is None:
        return ""
    s = s.strip()
    # collapse internal whitespace
    s = " ".join(s.split())
    # remove everything except letters/digits
    s = re.sub(r"[^0-9a-zA-Z]+", "", s)
    return s.lower()


def map_header_to_field(raw_header: str) -> str | None:
    """
    Map ANY weird header text to one of the 5 canonical fields if possible.
    Returns canonical field name or None if can't decide.
    """
    norm = normalize_text(raw_header)

    # Heuristic mapping based on keywords
    if "station" in norm and "timeperiod" not in norm:
        return "station"

    if "phenomenon" in norm:
        return "weatherPhenomenon"

    if "operationprobability" in norm or ("operation" in norm and "probability" in norm):
        return "operationProbability"

    if "timeperiod" in norm and "start" in norm:
        return "advisoryTimePeriodStartUTC"

    if "timeperiod" in norm and "end" in norm:
        return "advisoryTimePeriodEndUTC"

    # nothing matched
    return None


def sanitize_value(field: str, raw: str) -> str:
    """
    Clean up cell content but NEVER reject based on format.
    Just make it neat & consistent.
    """
    if raw is None:
        raw = ""

    # strip + collapse whitespace
    value = " ".join(str(raw).split())

    if field == "operationProbability":
        # remove % sign, keep digits only if present
        digits = re.findall(r"\d+", value)
        if digits:
            value = digits[0]  # e.g. "90%" -> "90"
    # For dates/times we just clean spaces; don't strictly validate:
    # advisoryTimePeriodStartUTC / advisoryTimePeriodEndUTC
    return value













def extract_advisory_from_html(html: str, subject: str, msg_id: str):
    """
    Returns (advisory_dict, error_reason)
    advisory_dict = {station, weatherPhenomenon, operationProbability,
                     advisoryTimePeriodStartUTC, advisoryTimePeriodEndUTC}
    If any of the 5 fields truly missing -> returns (None, reason)
    """
    soup = BeautifulSoup(html, "html.parser")

    tables = soup.find_all("table")
    if not tables:
        return None, "No <table> found in body"

    for t_index, table in enumerate(tables, start=1):
        # --- collect rows ---
        rows = table.find_all("tr")
        if len(rows) < 2:
            continue  # not enough rows to be a data table

        # --- try to build header mapping from first row that looks like headers ---
        header_cells = rows[0].find_all(["th", "td"])
        if not header_cells:
            continue

        header_map = {}  # canonical_field -> column_index
        for idx, cell in enumerate(header_cells):
            txt = cell.get_text(separator=" ", strip=True)
            field = map_header_to_field(txt)
            if field and field not in header_map:
                header_map[field] = idx

        # Check if all 5 canonical fields are present in this table
        missing = [f for f in CANONICAL_FIELDS if f not in header_map]
        if missing:
            # Not our advisory table, try next one
            continue

        # --- We found a table that has all required fields ---
        # Now take the FIRST data row that has a non-empty station.
        advisory = {}
        data_row_found = False

        for r in rows[1:]:
            cells = r.find_all(["td", "th"])
            if not cells:
                continue

            # get station cell text (may be blank in some rows)
            station_idx = header_map["station"]
            if station_idx >= len(cells):
                continue

            station_raw = cells[station_idx].get_text(separator=" ", strip=True)
            station_val = sanitize_value("station", station_raw)

            if not station_val:
                # empty station row (maybe header continuation), skip
                continue

            # We accept this row as the advisory row
            data_row_found = True
            advisory["station"] = station_val

            for field, col_idx in header_map.items():
                if field == "station":
                    continue
                if col_idx < len(cells):
                    raw_val = cells[col_idx].get_text(separator=" ", strip=True)
                else:
                    raw_val = ""
                advisory[field] = sanitize_value(field, raw_val)

            break  # we only need one advisory row

        if not data_row_found:
            # table had headers but no usable row
            return None, f"Table {t_index} had headers but no data row with station"

        # Final sanity: make sure all 5 keys exist even if blank strings
        for f in CANONICAL_FIELDS:
            advisory.setdefault(f, "")

        return advisory, None

    # If none of the tables satisfied the header check
    return None, "No table containing all 5 advisory headers"



























body_html = message["body"]["content"]   # from Graph with Prefer: html

advisory, err = extract_advisory_from_html(body_html, subject, msg_id)
if advisory is None:
    print(f"❌ Skipping email (id={msg_id}, subject='{subject}') – {err}")
    # log to file / hub etc.
    continue

print(f"✅ Advisory extracted from email '{subject}': {advisory}")
# yahan se advisory ko JSON bana ke hub ko bhej do
