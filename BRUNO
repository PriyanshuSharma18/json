import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timezone, timedelta
import time
import os
import re

# ========= Load Environment ========= #
def load_env(path: str = ".env"):
    if not os.path.exists(path):
        return
    with open(path, "r") as f:
        for line in f:
            if "=" in line:
                key, val = line.split("=", 1)
                os.environ[key.strip()] = val.strip().strip('"\'')
load_env()

ACCESS_TOKEN = os.getenv("ACCESS_TOKEN")
USER_EMAIL = os.getenv("USER_EMAIL")
IS_LOCAL_ENV = True

if not ACCESS_TOKEN or not USER_EMAIL:
    raise RuntimeError("ACCESS_TOKEN or USER_EMAIL missing in .env")

headers = {
    "Authorization": f"Bearer {ACCESS_TOKEN}",
    "Prefer": 'outlook.body-content-type="html"'
}

OUTPUT_DIR = "email_extracts"
os.makedirs(OUTPUT_DIR, exist_ok=True)

MONTH = {
    "jan": 1, "feb": 2, "mar": 3, "apr": 4, "may": 5, "jun": 6,
    "jul": 7, "aug": 8, "sep": 9, "oct": 10, "nov": 11, "dec": 12
}
IST = timezone(timedelta(hours=5, minutes=30))

MANDATORY = [
    "station", "weatherPhenomenon",
    "operationProbability",
    "advisoryTimePeriodStartUTC",
    "advisoryTimePeriodEndUTC"
]


# ========= TIME NORMALIZATION ========= #
def normalize_time(raw, year):
    """
    Convert time formats like:
    - 1500/23 Nov
    - 0030/24 Nov
    - ISO datetime
    into: YYYY-MM-DDTHH:MM:00Z
    """
    raw = raw.strip()

    # HHMM/DD MON
    m = re.match(r"(\d{3,4})/(\d{1,2})\s*([A-Za-z]{3})", raw)
    if m:
        hhmm, day, mon = m.groups()
        hour = int(hhmm[:-2])
        minute = int(hhmm[-2:])
        month = MONTH.get(mon.lower(), 1)
        dt_ist = datetime(year, month, int(day), hour, minute, tzinfo=IST)
        return dt_ist.astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

    # ISO style
    try:
        if "T" in raw:
            dt = datetime.fromisoformat(raw.replace("Z", "+00:00"))
            return dt.astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    except:
        return None

    return None


def utc_to_lt(utc_iso):
    try:
        dt = datetime.fromisoformat(utc_iso.replace("Z", "+00:00"))
        return dt.astimezone(IST).strftime("%Y-%m-%dT%H:%M:%SZ+05:30")
    except:
        return None


# ========= REQUIRED TEXT CHECK ========= #
def missing_fields(html):
    html_l = html.lower()
    checks = {
        "station": "station" in html_l,
        "weatherPhenomenon": "weather" in html_l and "phenom" in html_l,
        "operationProbability": "operat" in html_l and "prob" in html_l,
        "advisoryTimePeriodStartUTC": "advisory" in html_l and "start" in html_l,
        "advisoryTimePeriodEndUTC": "advisory" in html_l and "end" in html_l,
    }
    return [k for k, v in checks.items() if not v]


# ========= GRAPH API ========= #
def fetch_body(msg_id):
    url = f"https://graph.microsoft.com/v1.0/users/{USER_EMAIL}/messages/{msg_id}?$select=body"
    r = requests.get(url, headers=headers)
    return r.json().get("body", {}).get("content", "") if r.status_code == 200 else ""


# ========= NLP MAIN EXTRACTOR ========= #
def extract_stations(html, email_year):
    soup = BeautifulSoup(html, "html.parser")
    lines = [l.strip() for l in soup.get_text("\n", strip=True).splitlines() if l.strip()]

    final = []
    for i, line in enumerate(lines):

        # Station code
        if re.fullmatch(r"[A-Z]{3}", line):
            data = {"station": line}
            window = lines[i+1:i+20]

            # Operation Probability
            for w in window:
                if re.fullmatch(r"\d{1,3}", w) and 0 <= int(w) <= 100:
                    data["operationProbability"] = int(w)
                    break

            # Weather Phenomenon
            for w in window:
                if re.fullmatch(r"[A-Z]{2,6}", w) and w != line:
                    data["weatherPhenomenon"] = w
                    break

            # Times
            times = []
            lt_times = []

            for w in window:
                # Direct LT match first
                if re.search(r"LT|local", w, flags=re.IGNORECASE):
                    lt_times.append(w)

                # Normalize UTC candidate
                iso = normalize_time(w, email_year)
                if iso:
                    times.append(iso)

            # Assign UTC
            if len(times) >= 2:
                data["advisoryTimePeriodStartUTC"] = times[0]
                data["advisoryTimePeriodEndUTC"] = times[1]

                # LT direct vs fallback conversion
                data["advisoryTimePeriodStartLT"] = utc_to_lt(times[0])
                data["advisoryTimePeriodEndLT"] = utc_to_lt(times[1])

            # Validate 5 mandatory keys
            if all(k in data for k in MANDATORY):
                final.append(data)

    return final


# ========= MAIN PROCESSING ========= #
def process_messages():
    print("\nüì® Fetching recent emails...")
    url = f"https://graph.microsoft.com/v1.0/users/{USER_EMAIL}/messages?$top=50&$select=id,subject,receivedDateTime"
    r = requests.get(url, headers=headers).json()
    msgs = r.get("value", [])

    extracted = skipped = 0

    for idx, m in enumerate(msgs):
        print(f"\nüì© #{idx+1}: {m.get('subject')}")

        html = fetch_body(m["id"])
        if not html.strip():
            print("‚ùå No HTML body")
            skipped += 1
            continue

        missing = missing_fields(html)
        if missing:
            print(f"‚ö† Skip ‚Üí Missing: {', '.join(missing)}")
            skipped += 1
            continue

        email_year = int(m["receivedDateTime"][:4])
        stations = extract_stations(html, email_year)

        if not stations:
            print("‚ö† Skip ‚Üí No complete rows found")
            skipped += 1
            continue

        payload = {
            "createdAt": utc_to_lt(m["receivedDateTime"]),
            "stations": stations
        }

        fname = f"{idx+1:03d}_{m.get('subject','').replace(' ','_')[:25]}.json"
        with open(os.path.join(OUTPUT_DIR, fname), "w") as f:
            json.dump(payload, f, indent=2)

        print(f"‚úî Extracted {len(stations)} station(s) ‚Üí {fname}")
        extracted += 1

    print("\n====== SUMMARY ======")
    print(f"‚úî Success: {extracted}")
    print(f"‚ùå Skipped: {skipped}")
    print("üìÅ Saved:", OUTPUT_DIR)


if __name__ == "__main__":
    process_messages()
