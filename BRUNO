import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timezone, timedelta
import time
import os
import re

# ===================== ENV LOADING =====================

def load_env(path: str = ".env") -> None:
    """Minimal .env loader (no external dependency)."""
    if not os.path.exists(path):
        return

    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            if '=' in line:
                key, value = line.split('=', 1)
                key = key.strip()
                value = value.strip().strip('"\'')
                if key not in os.environ:
                    os.environ[key] = value

# Load .env early
load_env()

IS_LOCAL_ENV = os.environ.get("ENV", "").lower() == "local"

# ===================== CONFIG =====================

ACCESS_TOKEN = os.environ.get("ACCESS_TOKEN")
USER_EMAIL = os.environ.get("USER_EMAIL")

if not ACCESS_TOKEN:
    raise RuntimeError("ACCESS_TOKEN is not set. Please set it in .env or code.")
if not USER_EMAIL:
    raise RuntimeError("USER_EMAIL is not set. Please set it in .env or code.")

headers = {
    "Authorization": f"Bearer {ACCESS_TOKEN}",
    "Prefer": 'outlook.body-content-type="html"'
}

OUTPUT_DIR = "email_extracts"
if IS_LOCAL_ENV and not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

IST_OFFSET = timezone(timedelta(hours=5, minutes=30))

MONTH_MAP = {
    "jan": 1, "feb": 2, "mar": 3, "apr": 4,
    "may": 5, "jun": 6, "jul": 7, "aug": 8,
    "sep": 9, "oct": 10, "nov": 11, "dec": 12
}

# ===================== UTILS =====================

def sanitize_filename(filename: str) -> str:
    return re.sub(r'[<>:"/\\|?*]', '_', filename)

def convert_to_ist_format(utc_datetime_str: str) -> str:
    """
    Keep as-is (tumhari existing createdAt formatting).
    """
    try:
        utc_dt = datetime.fromisoformat(utc_datetime_str.replace('Z', '+00:00'))
        ist_dt = utc_dt.astimezone(IST_OFFSET)
        return ist_dt.strftime("%Y-%m-%dT%H:%M:%SZ+05:30")
    except Exception:
        ist_dt = datetime.now(IST_OFFSET)
        return ist_dt.strftime("%Y-%m-%dT%H:%M:%SZ+05:30")

def parse_mail_received_datetime(dt_str: str):
    """
    Mail ke receivedDateTime ko datetime object me convert karo (UTC aware).
    """
    if not dt_str:
        return None
    try:
        # Graph format: 2025-11-22T13:45:10Z
        return datetime.fromisoformat(dt_str.replace('Z', '+00:00')).astimezone(timezone.utc)
    except Exception:
        return None

def build_utc_from_dd_mon_hhmm(match_obj, mail_dt_utc):
    """
    '1500/23 Nov' jaisi string se proper UTC datetime banao.
    Year agar missing hai to mail ke year se lo.
    """
    try:
        hhmm = match_obj.group(1)   # 1500
        day = int(match_obj.group(2))  # 23
        mon_abbr = match_obj.group(3).lower()  # nov

        if len(hhmm) == 4:
            hour = int(hhmm[:2])
            minute = int(hhmm[2:])
        elif len(hhmm) == 3:
            hour = int(hhmm[:1])
            minute = int(hhmm[1:])
        else:
            return None

        month = MONTH_MAP.get(mon_abbr)
        if not month:
            return None

        # 1) Default year = mail receive year
        if mail_dt_utc is None:
            year = datetime.now(timezone.utc).year
        else:
            year = mail_dt_utc.year

            # Optional: agar advisory date mail date se bohot peeche lag rahi ho
            # to next year assume kar sakte ho (simple rule).
            if mail_dt_utc.month == 12 and month == 1:
                # Dec mail, Jan advisory -> next year
                year += 1

        dt_utc = datetime(year, month, day, hour, minute, 0, tzinfo=timezone.utc)
        return dt_utc
    except Exception:
        return None

def parse_advisory_times(window_lines, mail_received_dt_str):
    """
    Window ke andar se advisory ke liye:
      - Start UTC
      - End UTC
      - Start LT (IST)
      - End LT (IST)
    nikalta hai.

    Expect format: '1500/23 Nov' etc.
    """
    mail_dt_utc = parse_mail_received_datetime(mail_received_dt_str)

    matches = []
    pattern = re.compile(r'(\d{3,4})/(\d{1,2})\s*([A-Za-z]{3})')

    for line in window_lines:
        for m in pattern.finditer(line):
            matches.append(m)

    if len(matches) < 2:
        # Agar 2 se kam time blocks mile to skip
        return None

    start_dt_utc = build_utc_from_dd_mon_hhmm(matches[0], mail_dt_utc)
    end_dt_utc = build_utc_from_dd_mon_hhmm(matches[1], mail_dt_utc)

    if not start_dt_utc or not end_dt_utc:
        return None

    # Proper ISO 8601 UTC format: 2025-11-22T23:00:00Z
    start_utc_str = start_dt_utc.strftime("%Y-%m-%dT%H:%M:%SZ")
    end_utc_str = end_dt_utc.strftime("%Y-%m-%dT%H:%M:%SZ")

    # LT (IST) nikalna (agar mail me LT na mile toh bhi yeh kaafi hai)
    start_lt = start_dt_utc.astimezone(IST_OFFSET)
    end_lt = end_dt_utc.astimezone(IST_OFFSET)

    # Example: 2025-11-23T04:30:00+05:30
    start_lt_str = start_lt.isoformat(timespec="seconds")
    end_lt_str = end_lt.isoformat(timespec="seconds")

    return start_utc_str, end_utc_str, start_lt_str, end_lt_str

# ===================== FIELD LABEL CHECK =====================

def check_mandatory_fields_in_html(html_content: str):
    """
    Check that the HTML body conceptually contains 5 labels:
      station, weatherPhenomenon, operationProbability,
      advisoryTimePeriodStartUTC, advisoryTimePeriodEndUTC
    (only label presence check, values later NLP se nikalenge)
    """
    required_keys = {
        "station",
        "weatherPhenomenon",
        "operationProbability",
        "advisoryTimePeriodStartUTC",
        "advisoryTimePeriodEndUTC",
    }

    if not html_content or not html_content.strip():
        return list(required_keys)

    html_lower = html_content.lower()
    simple = re.sub(r'[^a-z0-9]', '', html_lower)

    present = set()

    # station
    if 'station' in html_lower or 'station' in simple:
        present.add("station")

    # weather phenomenon
    cond_weather = (
        ('weather' in html_lower and ('phenomenon' in html_lower or 'phenom' in html_lower))
        or 'weatherphenomenon' in simple
        or 'weatherphenom' in simple
    )
    if cond_weather:
        present.add("weatherPhenomenon")

    # operation / operational probability
    cond_op_prob = (
        (('operation' in html_lower or 'operational' in html_lower)
         and ('probability' in html_lower or 'probab' in html_lower))
        or 'operationprobability' in simple
        or 'operationalprobability' in simple
        or 'operationprobab' in simple
    )
    if cond_op_prob:
        present.add("operationProbability")

    # advisory start utc
    cond_start_utc = (
        ('advisory' in html_lower and 'start' in html_lower and 'utc' in html_lower)
        or 'advisorytimeperiodstartutc' in simple
        or 'timeperiodstartutc' in simple
        or 'periodstartutc' in simple
    )
    if cond_start_utc:
        present.add("advisoryTimePeriodStartUTC")

    # advisory end utc
    cond_end_utc = (
        ('advisory' in html_lower and 'end' in html_lower and 'utc' in html_lower)
        or 'advisorytimeperiodendutc' in simple
        or 'timeperiodendutc' in simple
        or 'periodendutc' in simple
    )
    if cond_end_utc:
        present.add("advisoryTimePeriodEndUTC")

    missing = [k for k in required_keys if k not in present]
    return missing

# ===================== NLP-STYLE EXTRACTION =====================

def extract_weather_stations_nlp(html_content: str, mail_received_dt: str = None):
    """
    NLP-style extractor:
    - HTML -> plain text -> lines
    - Find station lines (3-letter uppercase)
    - For each station, look in next few lines for:
      weatherPhenomenon, operationProbability,
      advisoryTimePeriodStartUTC, advisoryTimePeriodEndUTC,
      + advisoryTimePeriodStartLT, advisoryTimePeriodEndLT (new)
    Returns: list of station dicts (each with all 5 mandatory fields, plus LT).
    """
    soup = BeautifulSoup(html_content, "html.parser")
    text = soup.get_text("\n", strip=True)
    lines = [l.strip() for l in text.splitlines() if l.strip()]

    stations = []
    n = len(lines)
    i = 0

    while i < n:
        line = lines[i]

        # Station code = exactly 3 uppercase letters
        if re.fullmatch(r"[A-Z]{3}", line):
            station_code = line
            entry = {"station": station_code}

            # Look ahead in a window after this station
            window = lines[i+1: i+15]  # next up to 14 lines

            # ---- Find operationProbability (0‚Äì100) ----
            for w in window:
                m = re.search(r"\b([0-9]{1,3})\b", w)
                if m:
                    val = int(m.group(1))
                    if 0 <= val <= 100:
                        entry["operationProbability"] = val
                        break

            # ---- Find weatherPhenomenon (short uppercase word) ----
            for w in window:
                # pure uppercase 2‚Äì6 letters, not same as station, not just digits
                if re.fullmatch(r"[A-Z]{2,6}", w) and w != station_code:
                    entry["weatherPhenomenon"] = w
                    break

            # ---- Parse advisory time range (UTC + LT) from pattern '1500/23 Nov' ----
            time_result = parse_advisory_times(window, mail_received_dt)
            if time_result:
                (
                    start_utc_str,
                    end_utc_str,
                    start_lt_str,
                    end_lt_str
                ) = time_result

                entry["advisoryTimePeriodStartUTC"] = start_utc_str
                entry["advisoryTimePeriodEndUTC"] = end_utc_str

                # 3) LT bhi print karo (optional field, but ALWAYS output me present)
                entry["advisoryTimePeriodStartLT"] = start_lt_str
                entry["advisoryTimePeriodEndLT"] = end_lt_str

            # ---- Check all 5 mandatory fields (station, weatherPhenomenon, operationProbability, UTC start/end) ----
            mandatory = [
                "station",
                "weatherPhenomenon",
                "operationProbability",
                "advisoryTimePeriodStartUTC",
                "advisoryTimePeriodEndUTC",
            ]
            if all(k in entry for k in mandatory):
                stations.append(entry)

        i += 1

    return stations

# ===================== GRAPH API EMAIL FUNCTIONS =====================

def get_all_messages(page_size: int = 50, max_pages: int = None):
    url = (
        f"https://graph.microsoft.com/v1.0/users/{USER_EMAIL}/messages"
        f"?$top={page_size}"
        "&$orderby=receivedDateTime desc"
        "&$select=id,subject,receivedDateTime,from"
    )

    all_messages = []
    page_count = 0

    while url and (max_pages is None or page_count < max_pages):
        print(f"Fetching page {page_count + 1}...")
        resp = requests.get(url, headers=headers)

        if resp.status_code != 200:
            print(f"Error fetching messages: {resp.status_code}")
            print("Response:", resp.text)
            break

        data = resp.json()
        messages = data.get("value", [])
        all_messages.extend(messages)

        print(f"Retrieved {len(messages)} messages from page {page_count + 1}")

        url = data.get("@odata.nextLink")
        page_count += 1
        time.sleep(0.5)

    print(f"Total messages retrieved: {len(all_messages)}")
    return all_messages


def get_message_body_html(message_id: str) -> str:
    url = (
        f"https://graph.microsoft.com/v1.0/users/{USER_EMAIL}/messages/{message_id}"
        "?$select=subject,body"
    )

    resp = requests.get(url, headers=headers)
    if resp.status_code != 200:
        print(f"Error fetching message body for {message_id}: {resp.status_code}")
        print("Response:", resp.text)
        return ""

    data = resp.json()
    body = data.get("body", {})
    return body.get("content", "")

# ===================== MAIN PROCESSING =====================

def process_single_email(message):
    try:
        body_html = get_message_body_html(message["id"])
        if not body_html:
            print("   ‚ùå No HTML body found ‚Äì Skipping this email.")
            return None

        # 1) Check mandatory field labels present somewhere in body
        missing_fields = check_mandatory_fields_in_html(body_html)
        if missing_fields:
            print(f"   ‚ùå Missing mandatory field(s) in mail body: {', '.join(missing_fields)} ‚Äì Skipping this email.")
            return None

        print("   ‚úÖ All 5 mandatory field labels found in body ‚Äì running NLP extractor...")

        stations = extract_weather_stations_nlp(
            body_html,
            mail_received_dt=message.get("receivedDateTime", "")
        )

        if not stations:
            print("   ‚ùå NLP extractor could not find any complete stations ‚Äì Skipping this email.")
            return None

        weather_advisory = {
            "createdAt": convert_to_ist_format(message.get("receivedDateTime", "")),
            "stations": stations
        }
        return weather_advisory

    except Exception as e:
        print(f"   ‚ùå Error processing email: {e}")
        return None


def process_all_emails():
    if not IS_LOCAL_ENV:
        print("‚ùå File creation only available in local environment (ENV=local)")
        return 0, 0

    print("=" * 60)
    print("üå§Ô∏è  WEATHER ADVISORY EMAIL PROCESSOR ‚Äì NLP VERSION")
    print("=" * 60)
    print("üîç Uses label detection + NLP-style window parsing around station codes.")
    print("üö´ No artificial defaults ‚Äì only values present in mail are used\n")

    all_messages = get_all_messages(page_size=50)
    successful_extractions = 0
    skipped_emails = 0

    for idx, message in enumerate(all_messages):
        subject = message.get('subject', 'No Subject')[:80]
        print(f"\nProcessing email {idx + 1}/{len(all_messages)}: {subject}...")

        weather_advisory = process_single_email(message)

        if weather_advisory:
            if IS_LOCAL_ENV:
                subject_clean = sanitize_filename(message.get("subject", "No_Subject")[:30])
                date_clean = message.get("receivedDateTime", "")[:10].replace("-", "_")
                filename = f"{idx + 1:03d}_{date_clean}_{subject_clean}.json"
                filepath = os.path.join(OUTPUT_DIR, filename)

                with open(filepath, "w", encoding="utf-8") as f:
                    json.dump(weather_advisory, f, indent=2, ensure_ascii=False)

                station_count = len(weather_advisory["stations"])
                print(f"   ‚úÖ Extracted {station_count} station(s) ‚Äì Saved to {filename}")

            successful_extractions += 1
        else:
            skipped_emails += 1

        time.sleep(0.2)

    print("\n" + "=" * 60)
    print("üéØ EMAIL PROCESSING SUMMARY")
    print("=" * 60)
    print(f"üìß Total emails processed: {len(all_messages)}")
    print(f"‚úÖ Successful extractions: {successful_extractions}")
    print(f"‚è≠Ô∏è  Skipped emails: {skipped_emails}")
    print(f"üìÅ Files saved in: {OUTPUT_DIR}/")

    return len(all_messages), successful_extractions


def main():
    try:
        process_all_emails()
    except requests.HTTPError as e:
        print(f"‚ùå HTTP Error: {e}")
    except Exception as e:
        print(f"‚ùå Error: {e}")


if __name__ == "__main__":
    main()
