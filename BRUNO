import requests
import pandas as pd
from bs4 import BeautifulSoup
import json
from datetime import datetime, timezone, timedelta
import time
import os
import re

# ===================== ENV LOADING =====================

def load_env(path: str = ".env") -> None:
    """Minimal .env loader (no external dependency)."""
    if not os.path.exists(path):
        return

    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            if '=' in line:
                key, value = line.split('=', 1)
                key = key.strip()
                value = value.strip().strip('"\'')
                if key not in os.environ:
                    os.environ[key] = value

# Load .env early
load_env()

# Check environment
IS_LOCAL_ENV = os.environ.get("ENV", "").lower() == "local"

# ===================== CONFIG =====================

# Prefer taking from env; you can still hard-code if you want
ACCESS_TOKEN = os.environ.get("ACCESS_TOKEN")  # or "YOUR_ACCESS_TOKEN"
USER_EMAIL = os.environ.get("USER_EMAIL")      # or "xyz@contoso.com"

if not ACCESS_TOKEN:
    raise RuntimeError("ACCESS_TOKEN is not set. Please set it in .env or code.")
if not USER_EMAIL:
    raise RuntimeError("USER_EMAIL is not set. Please set it in .env or code.")

headers = {
    "Authorization": f"Bearer {ACCESS_TOKEN}",
    "Prefer": 'outlook.body-content-type="html"'
}

# Create output directory only for local environment
OUTPUT_DIR = "email_extracts"
if IS_LOCAL_ENV and not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ===================== UTILS =====================

def sanitize_filename(filename: str) -> str:
    """Remove invalid characters from filename."""
    return re.sub(r'[<>:"/\\|?*]', '_', filename)

def convert_to_ist_format(utc_datetime_str: str) -> str:
    """Convert UTC datetime string to IST format like in mod.json."""
    try:
        # Parse the UTC datetime
        utc_dt = datetime.fromisoformat(utc_datetime_str.replace('Z', '+00:00'))
        # Convert to IST (UTC+5:30)
        ist_offset = timezone(timedelta(hours=5, minutes=30))
        ist_dt = utc_dt.astimezone(ist_offset)
        return ist_dt.strftime("%Y-%m-%dT%H:%M:%SZ+05:30")
    except Exception:
        # Fallback to current IST time if parsing fails
        ist_offset = timezone(timedelta(hours=5, minutes=30))
        return datetime.now(ist_offset).strftime("%Y-%m-%dT%H:%M:%SZ+05:30")

# ===================== HTML PARSING =====================

def extract_tables_from_html_body(html_content: str):
    """
    Extract all table data from HTML body content.
    Returns list of dicts: {table_index, headers, data}
    where:
      headers = [header_text, ...]
      data    = [ {header_text: cell_value, ...}, ... ]
    """
    if not html_content or not html_content.strip():
        return []

    soup = BeautifulSoup(html_content, "html.parser")
    tables = soup.find_all("table")
    if not tables:
        return []

    extracted_tables = []

    for table_idx, table in enumerate(tables):
        table_rows = table.find_all("tr")
        if len(table_rows) < 2:  # Need at least header + 1 data row
            continue

        header_row = None
        data_rows = []

        for row in table_rows:
            cells = row.find_all(["td", "th"])
            if not cells:
                continue

            cell_texts = [cell.get_text(strip=True) for cell in cells]

            if header_row is None:
                header_row = cell_texts
            else:
                data_rows.append(cell_texts)

        if header_row and data_rows:
            table_data = []
            for row in data_rows:
                # Ensure row has same number of columns as header
                while len(row) < len(header_row):
                    row.append("")
                row_dict = {}
                for i, header in enumerate(header_row):
                    key = header.strip() if header else f"Column_{i+1}"
                    row_dict[key] = row[i].strip() if i < len(row) else ""
                table_data.append(row_dict)

            if table_data:
                extracted_tables.append({
                    "table_index": table_idx,
                    "headers": header_row,
                    "data": table_data
                })

    return extracted_tables

def check_mandatory_fields_in_html(html_content: str):
    """
    Check mandatory fields based on TABLE HEADERS, not raw text.
    Returns a list of missing conceptual fields (empty list => all present).

    Conceptual mandatory fields:
      - station
      - weatherPhenomenon
      - operationProbability
      - advisoryTimePeriodStartUTC
      - advisoryTimePeriodEndUTC
    """
    required_keys = {
        "station",
        "weatherPhenomenon",
        "operationProbability",
        "advisoryTimePeriodStartUTC",
        "advisoryTimePeriodEndUTC",
    }

    if not html_content or not html_content.strip():
        return list(required_keys)

    tables = extract_tables_from_html_body(html_content)
    if not tables:
        return list(required_keys)

    present_keys = set()

    for table in tables:
        headers = table.get("headers", [])
        for h in headers:
            h_lower = h.lower().strip()

            # station
            if "station" in h_lower:
                present_keys.add("station")

            # weather phenomenon
            if "weather" in h_lower and "phenomenon" in h_lower:
                present_keys.add("weatherPhenomenon")

            # operational / operation probability
            if ("operation" in h_lower or "operational" in h_lower) and "probability" in h_lower:
                present_keys.add("operationProbability")

            # advisory time period start utc
            if "advisory" in h_lower and "start" in h_lower and "utc" in h_lower:
                present_keys.add("advisoryTimePeriodStartUTC")

            # advisory time period end utc
            if "advisory" in h_lower and "end" in h_lower and "utc" in h_lower:
                present_keys.add("advisoryTimePeriodEndUTC")

    missing = [key for key in required_keys if key not in present_keys]
    return missing

def extract_weather_stations_from_tables(tables):
    """
    Extract weather station data from tables with original values.
    Only returns station entries where all 5 mandatory fields
    are present (no dummy values).
    """
    stations = []

    for table in tables:
        headers = table.get("headers", [])
        table_data = table.get("data", [])
        if not headers or not table_data:
            continue

        # Map headers -> conceptual fields (same logic as check_mandatory_fields_in_html)
        header_mapping = {}
        for i, header in enumerate(headers):
            h_lower = header.lower().strip()

            if "station" in h_lower:
                header_mapping["station"] = i

            if "weather" in h_lower and "phenomenon" in h_lower:
                header_mapping["weatherPhenomenon"] = i

            if ("operation" in h_lower or "operational" in h_lower) and "probability" in h_lower:
                header_mapping["operationProbability"] = i

            if "advisory" in h_lower and "start" in h_lower and "utc" in h_lower:
                header_mapping["advisoryTimePeriodStartUTC"] = i

            if "advisory" in h_lower and "end" in h_lower and "utc" in h_lower:
                header_mapping["advisoryTimePeriodEndUTC"] = i

            if "advisory" in h_lower and "start" in h_lower and ("lt" in h_lower or "local" in h_lower):
                header_mapping["advisoryTimePeriodStartLT"] = i

            if "advisory" in h_lower and "end" in h_lower and ("lt" in h_lower or "local" in h_lower):
                header_mapping["advisoryTimePeriodEndLT"] = i

        if not header_mapping:
            continue

        # Extract row-wise
        for row_dict in table_data:
            station_entry = {}

            for field_name, header_idx in header_mapping.items():
                if header_idx < len(headers):
                    header_name = headers[header_idx]
                    value = row_dict.get(header_name, "").strip()
                else:
                    value = ""

                if not value:
                    # Don't fill dummy values; just skip empty
                    continue

                if field_name == "station":
                    # Validate station: 3 uppercase letters
                    if len(value) == 3 and value.isalpha() and value.isupper():
                        station_entry[field_name] = value
                    else:
                        # Invalid station ‚Üí this row is probably not a proper station entry
                        continue
                elif field_name == "operationProbability":
                    try:
                        station_entry[field_name] = int(float(value))
                    except (ValueError, TypeError):
                        # Bad probability ‚Üí don't add dummy; skip
                        continue
                else:
                    # Use exact original text
                    station_entry[field_name] = value

            # Check that ALL 5 mandatory conceptual fields are present
            mandatory_fields = {
                "station",
                "weatherPhenomenon",
                "operationProbability",
                "advisoryTimePeriodStartUTC",
                "advisoryTimePeriodEndUTC",
            }

            if mandatory_fields.issubset(station_entry.keys()):
                stations.append(station_entry)

    return stations

# ===================== GRAPH API EMAIL FUNCTIONS =====================

def get_all_messages(page_size: int = 50, max_pages: int = None):
    """
    Get all messages from the mailbox with pagination.
    Returns a list of message objects with basic info.
    """
    url = (
        f"https://graph.microsoft.com/v1.0/users/{USER_EMAIL}/messages"
        f"?$top={page_size}"
        "&$orderby=receivedDateTime desc"
        "&$select=id,subject,receivedDateTime,from"
    )

    all_messages = []
    page_count = 0

    while url and (max_pages is None or page_count < max_pages):
        print(f"Fetching page {page_count + 1}...")
        resp = requests.get(url, headers=headers)

        if resp.status_code != 200:
            print(f"Error fetching messages: {resp.status_code}")
            print("Response:", resp.text)
            break

        data = resp.json()
        messages = data.get("value", [])
        all_messages.extend(messages)

        print(f"Retrieved {len(messages)} messages from page {page_count + 1}")

        # Next page
        url = data.get("@odata.nextLink")
        page_count += 1

        time.sleep(0.5)

    print(f"Total messages retrieved: {len(all_messages)}")
    return all_messages

def get_message_body_html(message_id: str) -> str:
    """
    Fetch full message body (HTML) for given message_id.
    """
    url = (
        f"https://graph.microsoft.com/v1.0/users/{USER_EMAIL}/messages/{message_id}"
        "?$select=subject,body"
    )

    resp = requests.get(url, headers=headers)
    if resp.status_code != 200:
        print(f"Error fetching message body for {message_id}: {resp.status_code}")
        print("Response:", resp.text)
        return ""

    data = resp.json()
    body = data.get("body", {})
    return body.get("content", "")

# ===================== MAIN PROCESSING =====================

def process_single_email(message):
    """
    Process a single email and extract weather advisory data if valid.
    Returns weather advisory dict or None if invalid/skip.
    """
    try:
        body_html = get_message_body_html(message["id"])
        if not body_html:
            print("   ‚ùå No HTML body found ‚Äì Skipping this email.")
            return None

        # 1) Check mandatory fields based on headers
        missing_fields = check_mandatory_fields_in_html(body_html)
        if missing_fields:
            print(f"   ‚ùå Missing mandatory field(s) in table headers: {', '.join(missing_fields)} ‚Äì Skipping this email.")
            return None

        print("   ‚úÖ All mandatory fields present in headers ‚Äì Extracting tables...")

        # 2) Extract tables
        tables = extract_tables_from_html_body(body_html)
        if not tables:
            print("   ‚ùå No tables found in HTML ‚Äì Skipping this email.")
            return None

        # 3) Extract stations
        stations = extract_weather_stations_from_tables(tables)
        if not stations:
            print("   ‚ùå No valid station rows with all 5 fields ‚Äì Skipping this email.")
            return None

        # 4) Build advisory JSON (original values only)
        weather_advisory = {
            "createdAt": convert_to_ist_format(message.get("receivedDateTime", "")),
            "stations": stations
        }

        return weather_advisory

    except Exception as e:
        print(f"   ‚ùå Error processing email: {e}")
        return None

def process_all_emails():
    """
    Process all emails and extract weather advisory data.
    Only processes emails with all 5 mandatory fields present in table headers.
    """
    if not IS_LOCAL_ENV:
        print("‚ùå File creation only available in local environment (ENV=local)")
        return 0, 0

    print("=" * 60)
    print("üå§Ô∏è  WEATHER ADVISORY EMAIL PROCESSOR V2")
    print("=" * 60)
    print("üîç HTML Table Header Validation Enabled")
    print("üìã Mandatory fields (from headers): station, weatherPhenomenon, operationProbability, advisoryTimePeriodStartUTC, advisoryTimePeriodEndUTC")
    print("üö´ No artificial defaults: only original email values are used\n")

    all_messages = get_all_messages(page_size=50)
    successful_extractions = 0
    skipped_emails = 0

    for idx, message in enumerate(all_messages):
        subject = message.get('subject', 'No Subject')[:80]
        print(f"\nProcessing email {idx + 1}/{len(all_messages)}: {subject}...")

        weather_advisory = process_single_email(message)

        if weather_advisory:
            if IS_LOCAL_ENV:
                subject_clean = sanitize_filename(message.get("subject", "No_Subject")[:30])
                date_clean = message.get("receivedDateTime", "")[:10].replace("-", "_")
                filename = f"{idx + 1:03d}_{date_clean}_{subject_clean}.json"
                filepath = os.path.join(OUTPUT_DIR, filename)

                with open(filepath, "w", encoding="utf-8") as f:
                    json.dump(weather_advisory, f, indent=2, ensure_ascii=False)

                station_count = len(weather_advisory["stations"])
                print(f"   ‚úÖ Extracted {station_count} valid station(s) ‚Äì Saved to {filename}")

            successful_extractions += 1
        else:
            skipped_emails += 1

        time.sleep(0.2)

    print("\n" + "=" * 60)
    print("üéØ EMAIL PROCESSING SUMMARY")
    print("=" * 60)
    print(f"üìß Total emails processed: {len(all_messages)}")
    print(f"‚úÖ Successful extractions: {successful_extractions}")
    print(f"‚è≠Ô∏è  Skipped emails: {skipped_emails}")
    print(f"üìÅ Files saved in: {OUTPUT_DIR}/")

    return len(all_messages), successful_extractions

def main():
    try:
        process_all_emails()
    except requests.HTTPError as e:
        print(f"‚ùå HTTP Error: {e}")
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main()
